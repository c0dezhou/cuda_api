//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-29745058
// Cuda compilation tools, release 11.3, V11.3.58
// Based on NVVM 7.0.1
//

.version 7.3
.target sm_52
.address_size 64

	// .globl	_Z20matrixMultiplyKernelPKfS0_Pfiii
.extern .shared .align 16 .b8 shared[];

.visible .entry _Z20matrixMultiplyKernelPKfS0_Pfiii(
	.param .u64 _Z20matrixMultiplyKernelPKfS0_Pfiii_param_0,
	.param .u64 _Z20matrixMultiplyKernelPKfS0_Pfiii_param_1,
	.param .u64 _Z20matrixMultiplyKernelPKfS0_Pfiii_param_2,
	.param .u32 _Z20matrixMultiplyKernelPKfS0_Pfiii_param_3,
	.param .u32 _Z20matrixMultiplyKernelPKfS0_Pfiii_param_4,
	.param .u32 _Z20matrixMultiplyKernelPKfS0_Pfiii_param_5
)
{
	.reg .pred 	%p<9>;
	.reg .f32 	%f<30>;
	.reg .b32 	%r<32>;
	.reg .b64 	%rd<34>;


	ld.param.u64 	%rd18, [_Z20matrixMultiplyKernelPKfS0_Pfiii_param_0];
	ld.param.u64 	%rd19, [_Z20matrixMultiplyKernelPKfS0_Pfiii_param_1];
	ld.param.u64 	%rd17, [_Z20matrixMultiplyKernelPKfS0_Pfiii_param_2];
	ld.param.u32 	%r14, [_Z20matrixMultiplyKernelPKfS0_Pfiii_param_3];
	ld.param.u32 	%r12, [_Z20matrixMultiplyKernelPKfS0_Pfiii_param_4];
	ld.param.u32 	%r13, [_Z20matrixMultiplyKernelPKfS0_Pfiii_param_5];
	cvta.to.global.u64 	%rd1, %rd19;
	cvta.to.global.u64 	%rd2, %rd18;
	mov.u32 	%r15, %ntid.x;
	mov.u32 	%r16, %ctaid.x;
	mov.u32 	%r17, %tid.x;
	mad.lo.s32 	%r1, %r16, %r15, %r17;
	mov.u32 	%r18, %ntid.y;
	mov.u32 	%r19, %ctaid.y;
	mov.u32 	%r20, %tid.y;
	mad.lo.s32 	%r2, %r19, %r18, %r20;
	setp.ge.s32 	%p1, %r2, %r14;
	setp.ge.s32 	%p2, %r1, %r12;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB0_9;

	setp.lt.s32 	%p4, %r13, 1;
	mov.f32 	%f29, 0f00000000;
	@%p4 bra 	$L__BB0_8;

	add.s32 	%r22, %r13, -1;
	and.b32  	%r31, %r13, 3;
	setp.lt.u32 	%p5, %r22, 3;
	mov.f32 	%f29, 0f00000000;
	mov.u32 	%r30, 0;
	@%p5 bra 	$L__BB0_5;

	sub.s32 	%r29, %r13, %r31;
	mul.wide.s32 	%rd20, %r1, 4;
	add.s64 	%rd31, %rd1, %rd20;
	mad.lo.s32 	%r24, %r13, %r2, 2;
	mul.wide.s32 	%rd21, %r24, 4;
	add.s64 	%rd30, %rd2, %rd21;
	mul.wide.s32 	%rd5, %r12, 4;
	mov.f32 	%f29, 0f00000000;
	mov.u32 	%r30, 0;

$L__BB0_4:
	ld.global.f32 	%f12, [%rd31];
	ld.global.f32 	%f13, [%rd30+-8];
	fma.rn.f32 	%f14, %f13, %f12, %f29;
	add.s64 	%rd22, %rd31, %rd5;
	ld.global.f32 	%f15, [%rd22];
	ld.global.f32 	%f16, [%rd30+-4];
	fma.rn.f32 	%f17, %f16, %f15, %f14;
	add.s64 	%rd23, %rd22, %rd5;
	ld.global.f32 	%f18, [%rd23];
	ld.global.f32 	%f19, [%rd30];
	fma.rn.f32 	%f20, %f19, %f18, %f17;
	add.s64 	%rd24, %rd23, %rd5;
	add.s64 	%rd31, %rd24, %rd5;
	ld.global.f32 	%f21, [%rd24];
	ld.global.f32 	%f22, [%rd30+4];
	fma.rn.f32 	%f29, %f22, %f21, %f20;
	add.s32 	%r30, %r30, 4;
	add.s64 	%rd30, %rd30, 16;
	add.s32 	%r29, %r29, -4;
	setp.ne.s32 	%p6, %r29, 0;
	@%p6 bra 	$L__BB0_4;

$L__BB0_5:
	setp.eq.s32 	%p7, %r31, 0;
	@%p7 bra 	$L__BB0_8;

	mad.lo.s32 	%r25, %r30, %r12, %r1;
	mul.wide.s32 	%rd25, %r25, 4;
	add.s64 	%rd33, %rd1, %rd25;
	mul.wide.s32 	%rd11, %r12, 4;
	mad.lo.s32 	%r26, %r13, %r2, %r30;
	mul.wide.s32 	%rd26, %r26, 4;
	add.s64 	%rd32, %rd2, %rd26;

$L__BB0_7:
	.pragma "nounroll";
	ld.global.f32 	%f23, [%rd33];
	ld.global.f32 	%f24, [%rd32];
	fma.rn.f32 	%f29, %f24, %f23, %f29;
	add.s64 	%rd33, %rd33, %rd11;
	add.s64 	%rd32, %rd32, 4;
	add.s32 	%r31, %r31, -1;
	setp.ne.s32 	%p8, %r31, 0;
	@%p8 bra 	$L__BB0_7;

$L__BB0_8:
	cvta.to.global.u64 	%rd27, %rd17;
	mad.lo.s32 	%r27, %r2, %r12, %r1;
	mul.wide.s32 	%rd28, %r27, 4;
	add.s64 	%rd29, %rd27, %rd28;
	st.global.f32 	[%rd29], %f29;

$L__BB0_9:
	ret;

}
	// .globl	_Z12bitonic_sortPi
.visible .entry _Z12bitonic_sortPi(
	.param .u64 _Z12bitonic_sortPi_param_0
)
{
	.reg .pred 	%p<10>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd2, [_Z12bitonic_sortPi_param_0];
	cvta.to.global.u64 	%rd3, %rd2;
	mov.u32 	%r1, %tid.x;
	mul.wide.u32 	%rd4, %r1, 4;
	add.s64 	%rd1, %rd3, %rd4;
	ld.global.u32 	%r16, [%rd1];
	shl.b32 	%r17, %r1, 2;
	mov.u32 	%r18, shared;
	add.s32 	%r2, %r18, %r17;
	st.shared.u32 	[%r2], %r16;
	bar.sync 	0;
	mov.u32 	%r3, %ntid.x;
	setp.lt.u32 	%p1, %r3, 2;
	@%p1 bra 	$L__BB1_13;

	mov.u32 	%r23, 2;

$L__BB1_2:
	setp.eq.s32 	%p2, %r23, 0;
	@%p2 bra 	$L__BB1_12;

	shr.u32 	%r24, %r23, 1;
	and.b32  	%r6, %r23, %r1;

$L__BB1_4:
	xor.b32  	%r8, %r24, %r1;
	setp.le.u32 	%p3, %r8, %r1;
	@%p3 bra 	$L__BB1_11;

	setp.ne.s32 	%p4, %r6, 0;
	shl.b32 	%r20, %r8, 2;
	add.s32 	%r9, %r18, %r20;
	@%p4 bra 	$L__BB1_8;

	ld.shared.u32 	%r10, [%r2];
	ld.shared.u32 	%r11, [%r9];
	setp.le.s32 	%p5, %r10, %r11;
	@%p5 bra 	$L__BB1_8;

	st.shared.u32 	[%r2], %r11;
	st.shared.u32 	[%r9], %r10;

$L__BB1_8:
	setp.eq.s32 	%p6, %r6, 0;
	@%p6 bra 	$L__BB1_11;

	ld.shared.u32 	%r12, [%r2];
	ld.shared.u32 	%r13, [%r9];
	setp.ge.s32 	%p7, %r12, %r13;
	@%p7 bra 	$L__BB1_11;

	st.shared.u32 	[%r2], %r13;
	st.shared.u32 	[%r9], %r12;

$L__BB1_11:
	bar.sync 	0;
	shr.u32 	%r24, %r24, 1;
	setp.ne.s32 	%p8, %r24, 0;
	@%p8 bra 	$L__BB1_4;

$L__BB1_12:
	shl.b32 	%r23, %r23, 1;
	setp.le.u32 	%p9, %r23, %r3;
	@%p9 bra 	$L__BB1_2;

$L__BB1_13:
	ld.shared.u32 	%r22, [%r2];
	st.global.u32 	[%rd1], %r22;
	ret;

}
	// .globl	_Z12binarySearchPKiiPii
.visible .entry _Z12binarySearchPKiiPii(
	.param .u64 _Z12binarySearchPKiiPii_param_0,
	.param .u32 _Z12binarySearchPKiiPii_param_1,
	.param .u64 _Z12binarySearchPKiiPii_param_2,
	.param .u32 _Z12binarySearchPKiiPii_param_3
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<22>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd2, [_Z12binarySearchPKiiPii_param_0];
	ld.param.u32 	%r9, [_Z12binarySearchPKiiPii_param_1];
	ld.param.u64 	%rd3, [_Z12binarySearchPKiiPii_param_2];
	ld.param.u32 	%r10, [_Z12binarySearchPKiiPii_param_3];
	setp.lt.s32 	%p1, %r10, 1;
	mov.u32 	%r11, -1;
	mov.u32 	%r21, %r11;
	@%p1 bra 	$L__BB2_4;

	cvta.to.global.u64 	%rd1, %rd2;
	add.s32 	%r19, %r10, -1;
	mov.u32 	%r20, 0;

$L__BB2_2:
	add.s32 	%r13, %r19, %r20;
	shr.u32 	%r14, %r13, 31;
	add.s32 	%r15, %r13, %r14;
	shr.s32 	%r21, %r15, 1;
	mul.wide.s32 	%rd4, %r21, 4;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.u32 	%r5, [%rd5];
	setp.eq.s32 	%p2, %r5, %r9;
	@%p2 bra 	$L__BB2_4;

	setp.lt.s32 	%p3, %r5, %r9;
	add.s32 	%r17, %r21, 1;
	selp.b32 	%r20, %r17, %r20, %p3;
	add.s32 	%r18, %r21, -1;
	selp.b32 	%r19, %r19, %r18, %p3;
	setp.le.s32 	%p4, %r20, %r19;
	mov.u32 	%r21, %r11;
	@%p4 bra 	$L__BB2_2;

$L__BB2_4:
	cvta.to.global.u64 	%rd6, %rd3;
	st.global.u32 	[%rd6], %r21;
	ret;

}
	// .globl	_Z7forwardPfS_i
.visible .entry _Z7forwardPfS_i(
	.param .u64 _Z7forwardPfS_i_param_0,
	.param .u64 _Z7forwardPfS_i_param_1,
	.param .u32 _Z7forwardPfS_i_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [_Z7forwardPfS_i_param_0];
	ld.param.u64 	%rd2, [_Z7forwardPfS_i_param_1];
	ld.param.u32 	%r2, [_Z7forwardPfS_i_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB3_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	mul.f32 	%f2, %f1, %f1;
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f32 	[%rd7], %f2;

$L__BB3_2:
	ret;

}
	// .globl	_Z8backwardPfS_i
.visible .entry _Z8backwardPfS_i(
	.param .u64 _Z8backwardPfS_i_param_0,
	.param .u64 _Z8backwardPfS_i_param_1,
	.param .u32 _Z8backwardPfS_i_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<16>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [_Z8backwardPfS_i_param_0];
	ld.param.u64 	%rd2, [_Z8backwardPfS_i_param_1];
	ld.param.u32 	%r2, [_Z8backwardPfS_i_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB4_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	abs.f32 	%f2, %f1;
	lg2.approx.f32 	%f3, %f2;
	mul.f32 	%f4, %f3, 0f3EAAAAAB;
	ex2.approx.ftz.f32 	%f5, %f4;
	mul.f32 	%f6, %f5, %f5;
	rcp.approx.ftz.f32 	%f7, %f6;
	neg.f32 	%f8, %f2;
	fma.rn.f32 	%f9, %f7, %f8, %f5;
	mov.f32 	%f10, 0fBEAAAAAB;
	fma.rn.f32 	%f11, %f9, %f10, %f5;
	mov.b32 	%r6, %f1;
	setp.lt.s32 	%p2, %r6, 0;
	neg.f32 	%f12, %f11;
	selp.f32 	%f13, %f12, %f11, %p2;
	add.f32 	%f14, %f1, %f1;
	setp.eq.f32 	%p3, %f14, %f1;
	selp.f32 	%f15, %f14, %f13, %p3;
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f32 	[%rd7], %f15;

$L__BB4_2:
	ret;

}

